Import the Dependencies

import numpy as np
import pandas as pd
import sklearn
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from pylab import rcParams
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report
rcParams['figure.figsize'] = 14, 8
RANDOM_SEED = 42
LABELS = ["Normal", "Fraud"]





# load dataset to pandas dataset
credit_card_data = pd.read_csv('creditcard.csv')

# first 5 rows of the dataset
credit_card_data.head()

# last 5 rows of the dataset
credit_card_data.tail()

# dataset information
credit_card_data.info()

credit_card_data.isnull().values.any()

# checking the missing values in each column
credit_card_data.isnull().sum()

# distribution of legit transcation and fraudulant transaction
credit_card_data['Class'].value_counts()

This Dataset is a Highly Unbalanced Dataset

0 → Legit transactions

1 → Fraudulant transaction

# separeting the data for analysis
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]

print(legit.shape)
print(fraud.shape)

# statistical measures of the dataset
legit.Amount.describe()

fraud.Amount.describe()

# compare the values for both transactions
credit_card_data.groupby('Class').mean()

Under - Sampling

Build a sample dataset containing similar distribution of normal transactions and Fraudulent Transactions

Number of Fraudulent Transactions → 81

legit_sample = legit.sample(n=81)

Concatenating two data frames

new_dataset = pd.concat([legit_sample, fraud])

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

new_dataset.groupby('Class').mean()

new_dataset.groupby('Class').mean()

Spliting the data into Features and targets

X = new_dataset.drop(columns='Class')
Y = new_dataset['Class']

print(X)

print(Y)

Split the data into Training data and Testing data

X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

print(Y.shape, Y_train.shape, Y_test.shape)

Model Training

Logistic Regression

model = LogisticRegression()

# training the LogisticRegression with training data
model.fit(X_train, Y_train)

Model Evaluation

Accurecy Score

# accurecy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ' , training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score (X_test_prediction, Y_test)

print('Accuracy score on Test Data : ', test_data_accuracy)

count_classes = pd.value_counts(credit_card_data['Class'], sort = True)

count_classes.plot(kind = 'bar', rot=0)

plt.title("Transaction Class Distribution")

plt.xticks(range(2), LABELS)

plt.xlabel("Class")

plt.ylabel("Frequency")

## Get the Fraud and the normal dataset

fraud = credit_card_data[credit_card_data['Class']==1]

normal = credit_card_data[credit_card_data['Class']==0]

print(fraud.shape,normal.shape)

# We need to analyze more amount of information from the transaction data
#How different are the amount of money used in different transaction classes?
fraud.Amount.describe()


normal.Amount.describe()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Amount per transaction by class')
bins = 50
ax1.hist(fraud.Amount, bins = bins)
ax1.set_title('Fraud')
ax2.hist(normal.Amount, bins = bins)
ax2.set_title('Normal')
plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.xlim((0, 20000))
plt.yscale('log')
plt.show();

from IPython.core.interactiveshell import Any
# We Will check Do fraudulent transactions occur more often during certain time frame ? Let us find out with a visual representation.

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Time of transaction vs Amount by class')
ax1.set_title('Fraud')
ax1.scatter(fraud.Time, fraud.Amount)
ax2.set_title('Normal')
ax2.scatter(fraud.Time, fraud.Amount)
plt.xlabel('Time (in Seconds)')
plt.ylabel('Amount')
plt.show()

# Take some sample of the data

data1= credit_card_data.sample(frac = 0.1,random_state=1)

data1.shape

#Determine the number of fraud and valid transactions in the dataset

Fraud = data1[data1['Class']==1]

Valid = data1[data1['Class']==0]

outlier_fraction = len(Fraud)/float(len(Valid))

print(outlier_fraction)

print("Fraud Cases : {}".format(len(Fraud)))

print("Valid Cases : {}".format(len(Valid)))

# Correlation
import seaborn as sns
#get correlations of each features in dataset
data1= credit_card_data.sample(frac = 0.1,random_state=1)
corrmat = data1.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(credit_card_data[top_corr_features].corr(),annot=True,cmap="RdYlGn")

#Create independent and Dependent Features
columns = data1.columns.tolist()
# Filter the columns to remove data we do not want
columns = [c for c in columns if c not in ["Class"]]
# Store the variable we are predicting
target = "Class"
# Define a random state
state = np.random.RandomState(42)
X = data1[columns]
Y = data1[target]
X_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))
# Print the shapes of X & Y
print(X.shape)
print(Y.shape)
